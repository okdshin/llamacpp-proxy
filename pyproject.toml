[project]
name = "llamacpp-proxy"
version = "0.1.0"
description = "OpenAI API compatible reverse proxy for llama.cpp server"
authors = [
    {name = "okdshin", email = "kokuzen@gmail.com"},
]
readme = "README.md"
requires-python = ">=3.8"
license = "MIT"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "fastapi>=0.109.0",
    "uvicorn>=0.27.0",
    "httpx>=0.26.0",
    "jinja2>=3.1.0",
    "pydantic>=2.0.0",
    "typer>=0.9.0",
    "python-dotenv>=1.0.0",
    "pydantic-settings>=2.1.0",
]

[project.scripts]
llamacpp-proxy = "llamacpp_proxy.main:main"

[build-system]
requires = ["hatchling>=1.21.0"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/llamacpp_proxy"]

[tool.hatch.build.targets.sdist]
include = [
    "/src",
]
